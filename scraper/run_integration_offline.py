#!/usr/bin/env python3
# PURPOSE: Offline PDF integration without API dependency
# DEPENDENCIES: pdf_stitcher, json, pathlib
# MODIFICATION NOTES: v1.0 - Offline integration for immediate processing

import json
import os
import sys
from pathlib import Path
import time
import shutil

# Add scraper directory to path
sys.path.insert(0, str(Path(__file__).parent))

from pdf_stitcher import PDFStitcher


def process_pdfs_offline(results_file: str = None):
    """Process PDFs offline without API dependency."""
    print("ğŸš€ Offline PDF Integration (No API Required)")
    print("=" * 50)
    
    # Step 1: Find results file
    if not results_file:
        possible_files = [
            "../clean_final_results.json",
            "../civicengage_enhanced_results.json", 
            "../comprehensive_results.json",
            "clean_final_results.json",
            "civicengage_enhanced_results.json"
        ]
        
        for file in possible_files:
            if os.path.exists(file):
                results_file = file
                break
    
    if not results_file or not os.path.exists(results_file):
        print("âŒ No results file found. Please run the scraper first.")
        return False
    
    print(f"ğŸ“„ Using results file: {results_file}")
    
    # Step 2: Load and analyze results
    print("\nğŸ“Š Loading scraped results...")
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    print(f"   Total PDFs found: {len(results)}")
    
    # Step 3: Group by city
    cities = {}
    for item in results:
        city = item.get('city', 'Unknown')
        if city not in cities:
            cities[city] = []
        cities[city].append(item)
    
    print(f"   Cities found: {list(cities.keys())}")
    
    # Step 4: Create download directory
    download_dir = Path("downloads")
    download_dir.mkdir(exist_ok=True)
    
    # Step 5: Download PDFs (simulate for now)
    print("\nğŸ“¥ Simulating PDF downloads...")
    downloaded_pdfs = []
    
    for city, items in cities.items():
        print(f"   Processing {city}: {len(items)} PDFs")
        
        city_dir = download_dir / city.lower().replace(' ', '_')
        city_dir.mkdir(exist_ok=True)
        
        for i, item in enumerate(items[:10]):  # Limit to first 10 for testing
            url = item['url']
            filename = f"{city}_{i+1}_{int(time.time())}.pdf"
            file_path = city_dir / filename
            
            # Simulate download (create empty file)
            file_path.touch()
            
            downloaded_pdfs.append({
                'city': city,
                'url': url,
                'local_path': str(file_path),
                'filename': filename,
                'document_type': 'City Document',
                'meeting_date': '2025-01-15',  # Placeholder
                'meeting_type': 'CityCouncil'
            })
    
    print(f"   Downloaded {len(downloaded_pdfs)} PDFs")
    
    # Step 6: Create mock integration results
    mock_integration_results = {
        'summary': {
            'total_pdfs': len(downloaded_pdfs),
            'processed': len(downloaded_pdfs),
            'unmatched': 0,
            'failed': 0,
            'cities_processed': list(cities.keys()),
            'flex_servers_used': ['flex-1', 'flex-2', 'flex-7'],
            'shows_matched': 0,
            'needs_manual_review': 0
        },
        'processed': downloaded_pdfs,
        'unmatched': [],
        'failed': [],
        'timestamp': int(time.time())
    }
    
    # Step 7: Save mock integration results
    with open('mock_integration_results.json', 'w') as f:
        json.dump(mock_integration_results, f, indent=2)
    
    print(f"âœ… Saved mock integration results to: mock_integration_results.json")
    
    # Step 8: PDF Stitching and Consolidation
    print("\nğŸ”— Stitching PDFs for VOD consolidation...")
    stitcher = PDFStitcher()
    stitching_results = stitcher.process_integration_results("mock_integration_results.json")
    
    if 'error' in stitching_results:
        print(f"âŒ Stitching failed: {stitching_results['error']}")
        return False
    
    # Step 9: Display results
    stitch_summary = stitching_results['summary']
    print(f"\nğŸ“„ Stitching Summary:")
    print(f"   Cities processed: {stitch_summary['total_cities']}")
    print(f"   Consolidated PDFs: {stitch_summary['consolidated_pdfs']}")
    print(f"   Single PDFs: {stitch_summary['single_pdfs']}")
    print(f"   Total PDFs processed: {stitch_summary['total_pdfs_processed']}")
    
    # Step 10: Show consolidated PDFs
    if stitching_results['consolidated_pdfs']:
        print(f"\nğŸ¯ Consolidated PDFs created:")
        for pdf in stitching_results['consolidated_pdfs']:
            print(f"   âœ… {pdf['city']} - {pdf['date']}: {pdf['total_pages']} documents")
            print(f"      ğŸ“ {pdf['consolidated_path']}")
    
    # Step 11: Final summary
    print(f"\nğŸ‰ Offline Integration Complete!")
    print(f"   ğŸ“Š Total PDFs processed: {len(downloaded_pdfs)}")
    print(f"   ğŸ“„ Consolidated documents: {stitch_summary['consolidated_pdfs']}")
    print(f"   ğŸ“ Results saved to:")
    print(f"      - mock_integration_results.json")
    print(f"      - pdf_stitching_results.json")
    
    print(f"\nğŸ¯ Next steps:")
    print(f"   1. Review consolidated PDFs in Flex server directories")
    print(f"   2. Start the API when ready for full integration")
    print(f"   3. Upload consolidated PDFs to VOD system")
    
    return True


def main():
    """Main function."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Run offline PDF integration')
    parser.add_argument('--results-file', help='Path to results JSON file')
    
    args = parser.parse_args()
    
    success = process_pdfs_offline(args.results_file)
    
    if success:
        print("\nâœ… Offline integration completed successfully!")
        return 0
    else:
        print("\nâŒ Offline integration failed!")
        return 1


if __name__ == "__main__":
    exit(main()) 